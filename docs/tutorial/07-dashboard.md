# Chapter 7: Dashboard Development

## Overview

This chapter guides you through building an interactive Streamlit dashboard that provides real-time bike demand forecasts, model performance monitoring, and system health visibility.

**What You'll Build**:
- Multi-page Streamlit application
- Real-time demand forecast visualizations
- Model performance tracking dashboard
- Data quality monitoring interface
- System health status page

**Estimated Time**: 2-3 hours

## Why Streamlit?

**Decision**: Streamlit vs. React/Vue/Angular

**Why Streamlit Won**:
- **Rapid Development**: Build interactive dashboards in pure Python (no HTML/CSS/JavaScript)
- **Data Science Focus**: Built-in support for Plotly, pandas, ML model visualization
- **Hot Reload**: Changes appear instantly without rebuilding
- **Authentication**: Built-in auth for production deployments
- **Time to Market**: 10x faster than traditional web frameworks

**Trade-offs**:
- Less customizable than React
- Performance limits with large datasets (handled with caching)
- Best for internal tools, not customer-facing apps

**Result**: Dashboard built in ~300 lines of Python vs. ~2,000 lines with React.

---

## Architecture

```
dashboard/
â”œâ”€â”€ app.py                          # Main entry point
â”œâ”€â”€ pages/                          # Multi-page app
â”‚   â”œâ”€â”€ 1_ğŸ“ˆ_demand_forecast.py    # Station demand predictions
â”‚   â”œâ”€â”€ 2_ğŸ¯_model_performance.py  # Model metrics tracking
â”‚   â”œâ”€â”€ 3_ğŸ“Š_data_quality.py       # Pipeline monitoring
â”‚   â””â”€â”€ 4_ğŸ’š_system_health.py      # Health checks
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ charts.py                   # Reusable Plotly charts
â”‚   â”œâ”€â”€ api_client.py               # FastAPI integration
â”‚   â””â”€â”€ styling.py                  # Custom CSS
â””â”€â”€ config.py                       # Dashboard configuration
```

**Navigation Flow**:
1. User selects page from sidebar
2. Page fetches data from FastAPI or PostgreSQL
3. Data rendered with Plotly charts
4. Auto-refresh every 30 seconds (configurable)

---

## Step 1: Main Application Entry Point

### `dashboard/app.py`

```python
import streamlit as st
from pathlib import Path

# Page configuration (MUST be first Streamlit command)
st.set_page_config(
    page_title="Bike Demand Forecasting",
    page_icon="ğŸš´",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for better styling
def load_custom_css():
    st.markdown("""
        <style>
        .main {
            padding: 0rem 1rem;
        }
        .stMetric {
            background-color: #f0f2f6;
            padding: 10px;
            border-radius: 5px;
        }
        .stMetric label {
            font-size: 14px !important;
        }
        .stMetric .metric-value {
            font-size: 28px !important;
            font-weight: bold;
        }
        h1 {
            color: #1f77b4;
            padding-bottom: 10px;
            border-bottom: 2px solid #1f77b4;
        }
        .sidebar .sidebar-content {
            background-color: #f8f9fa;
        }
        </style>
    """, unsafe_allow_html=True)

load_custom_css()

# Sidebar
with st.sidebar:
    st.image("https://via.placeholder.com/150x50?text=BikeShare", width=150)
    st.title("ğŸš´ Bike Demand Forecasting")
    st.markdown("---")
    st.markdown("""
    **System Status**: ğŸŸ¢ Online

    **Features**:
    - ğŸ“ˆ Real-time Forecasts
    - ğŸ¯ Model Performance
    - ğŸ“Š Data Quality Monitoring
    - ğŸ’š System Health

    **Tech Stack**:
    - FastAPI Backend
    - XGBoost/LightGBM Models
    - PostgreSQL Database
    - MLflow Model Registry
    """)

    st.markdown("---")
    st.info("ğŸ’¡ **Tip**: Use the navigation above to explore different views.")

# Main page content
st.title("ğŸ  Welcome to Bike Demand Forecasting Dashboard")

st.markdown("""
## ğŸ¯ About This System

This dashboard provides **real-time bike demand predictions** for NYC Citi Bike stations using machine learning.

### ğŸ“Š Key Features

**1. Demand Forecasting**
- Predict bike availability 1-6 hours ahead
- Station-level granularity
- Weather-aware predictions

**2. Model Performance**
- Track RMSE, MAE, MAPE metrics
- Historical performance trends
- Model comparison (XGBoost vs LightGBM)

**3. Data Quality**
- Monitor data freshness
- Track missing data patterns
- Validate feature distributions

**4. System Health**
- API response time monitoring
- Database connection status
- Model serving availability

### ğŸš€ Quick Start

1. Navigate to **ğŸ“ˆ Demand Forecast** to see predictions
2. Check **ğŸ¯ Model Performance** for accuracy metrics
3. Monitor **ğŸ“Š Data Quality** for pipeline health
4. Review **ğŸ’š System Health** for system status

---

### ğŸ“ˆ System Metrics (Last 24 Hours)
""")

# Display quick metrics (fetch from API or database)
col1, col2, col3, col4 = st.columns(4)

with col1:
    st.metric(
        label="Total Predictions",
        value="12,453",
        delta="+1,234 from yesterday"
    )

with col2:
    st.metric(
        label="Average RMSE",
        value="0.51 bikes",
        delta="-0.03 (improved)"
    )

with col3:
    st.metric(
        label="API Uptime",
        value="99.8%",
        delta="+0.1%"
    )

with col4:
    st.metric(
        label="Data Freshness",
        value="< 5 min",
        delta="Healthy"
    )

st.markdown("---")
st.success("âœ… All systems operational. Navigate using the sidebar to explore detailed views.")
```

**Why This Structure?**
- **`set_page_config()` first**: Streamlit requirement - must be the first command
- **Custom CSS**: Improves visual appeal beyond default Streamlit styling
- **Sidebar navigation**: Provides context and quick system status
- **Main page metrics**: Dashboard landing page with high-level KPIs
- **Clear call-to-action**: Guides users to explore specific pages

---

## Step 2: Demand Forecast Page

### `dashboard/pages/1_ğŸ“ˆ_demand_forecast.py`

```python
import streamlit as st
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime, timedelta
import requests
from typing import Dict, List

st.set_page_config(page_title="Demand Forecast", page_icon="ğŸ“ˆ", layout="wide")

st.title("ğŸ“ˆ Bike Demand Forecast")
st.markdown("Real-time predictions for bike availability at NYC Citi Bike stations")

# Configuration
API_BASE_URL = "http://localhost:8000"  # FastAPI backend

# --- Helper Functions ---

@st.cache_data(ttl=300)  # Cache for 5 minutes
def fetch_stations() -> List[Dict]:
    """Fetch list of bike stations from API"""
    try:
        response = requests.get(f"{API_BASE_URL}/stations", timeout=10)
        response.raise_for_status()
        return response.json()
    except Exception as e:
        st.error(f"âŒ Failed to fetch stations: {e}")
        return []

@st.cache_data(ttl=60)  # Cache for 1 minute
def fetch_forecast(station_id: str, hours: int = 6) -> Dict:
    """Fetch demand forecast from API"""
    try:
        response = requests.get(
            f"{API_BASE_URL}/predict/station/{station_id}/forecast",
            params={"hours": hours},
            timeout=10
        )
        response.raise_for_status()
        return response.json()
    except Exception as e:
        st.error(f"âŒ Failed to fetch forecast: {e}")
        return None

def fetch_current_status(station_id: str) -> Dict:
    """Fetch current station status"""
    try:
        response = requests.get(
            f"{API_BASE_URL}/stations/{station_id}/current",
            timeout=5
        )
        response.raise_for_status()
        return response.json()
    except Exception as e:
        return {"bikes_available": 0, "docks_available": 0, "capacity": 20}

def create_forecast_chart(forecast_data: Dict) -> go.Figure:
    """Create Plotly chart for demand forecast"""
    forecasts = forecast_data.get("forecasts", [])

    # Prepare data
    timestamps = [f["timestamp"] for f in forecasts]
    predicted = [f["predicted_demand"] for f in forecasts]
    ci_lower = [f["confidence_interval_lower"] for f in forecasts]
    ci_upper = [f["confidence_interval_upper"] for f in forecasts]

    # Create figure
    fig = go.Figure()

    # Add predicted demand line
    fig.add_trace(go.Scatter(
        x=timestamps,
        y=predicted,
        mode='lines+markers',
        name='Predicted Demand',
        line=dict(color='#1f77b4', width=3),
        marker=dict(size=8)
    ))

    # Add confidence interval
    fig.add_trace(go.Scatter(
        x=timestamps + timestamps[::-1],
        y=ci_upper + ci_lower[::-1],
        fill='toself',
        fillcolor='rgba(31, 119, 180, 0.2)',
        line=dict(color='rgba(255,255,255,0)'),
        name='80% Confidence Interval',
        showlegend=True
    ))

    # Layout
    fig.update_layout(
        title="ğŸ“ˆ Bike Demand Forecast (Next 6 Hours)",
        xaxis_title="Time",
        yaxis_title="Predicted Bikes Available",
        hovermode='x unified',
        template='plotly_white',
        height=500,
        legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1
        )
    )

    return fig

def create_demand_heatmap(station_id: str) -> go.Figure:
    """Create hourly demand heatmap for a week"""
    # This would fetch historical data for the station
    # For demo, using synthetic data

    days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
    hours = list(range(24))

    # Synthetic demand pattern (replace with actual data query)
    import numpy as np
    demand = np.random.randint(5, 20, size=(7, 24))

    # Add realistic patterns
    # Morning rush (7-9 AM)
    demand[:5, 7:9] += 10  # Weekdays
    # Evening rush (5-7 PM)
    demand[:5, 17:19] += 12
    # Weekend afternoon peak
    demand[5:7, 13:16] += 8

    fig = go.Figure(data=go.Heatmap(
        z=demand,
        x=hours,
        y=days,
        colorscale='Blues',
        hoverongaps=False,
        hovertemplate='%{y}, %{x}:00<br>Demand: %{z} bikes<extra></extra>'
    ))

    fig.update_layout(
        title="ğŸ“… Weekly Demand Pattern (Avg Bikes Available by Hour)",
        xaxis_title="Hour of Day",
        yaxis_title="Day of Week",
        height=400
    )

    return fig

# --- Main UI ---

# Station selector
stations = fetch_stations()

if not stations:
    st.error("âŒ Cannot load stations. Please check API connection.")
    st.stop()

station_options = {f"{s['name']} ({s['station_id']})": s['station_id'] for s in stations}
selected_station_display = st.selectbox(
    "ğŸš‰ Select Station",
    options=list(station_options.keys())
)
selected_station_id = station_options[selected_station_display]

# Settings
col1, col2, col3 = st.columns([2, 2, 1])

with col1:
    forecast_hours = st.slider("Forecast Horizon (hours)", 1, 12, 6)

with col2:
    auto_refresh = st.checkbox("ğŸ”„ Auto-refresh (30s)", value=False)

with col3:
    if st.button("ğŸ”„ Refresh Now"):
        st.cache_data.clear()
        st.rerun()

# Auto-refresh logic
if auto_refresh:
    import time
    time.sleep(30)
    st.rerun()

st.markdown("---")

# Current Status
st.subheader("ğŸ“ Current Station Status")

current_status = fetch_current_status(selected_station_id)

col1, col2, col3, col4 = st.columns(4)

with col1:
    st.metric(
        label="ğŸš´ Bikes Available",
        value=current_status["bikes_available"],
        delta=None
    )

with col2:
    st.metric(
        label="ğŸ…¿ï¸ Docks Available",
        value=current_status["docks_available"],
        delta=None
    )

with col3:
    st.metric(
        label="ğŸ“Š Total Capacity",
        value=current_status["capacity"],
        delta=None
    )

with col4:
    utilization = (current_status["bikes_available"] / current_status["capacity"]) * 100
    st.metric(
        label="ğŸ“ˆ Utilization",
        value=f"{utilization:.1f}%",
        delta=None
    )

st.markdown("---")

# Forecast
st.subheader(f"ğŸ”® Demand Forecast (Next {forecast_hours} Hours)")

forecast_data = fetch_forecast(selected_station_id, hours=forecast_hours)

if forecast_data:
    # Main forecast chart
    fig_forecast = create_forecast_chart(forecast_data)
    st.plotly_chart(fig_forecast, use_container_width=True)

    # Forecast summary table
    st.subheader("ğŸ“‹ Detailed Forecast")

    forecast_df = pd.DataFrame(forecast_data["forecasts"])
    forecast_df["timestamp"] = pd.to_datetime(forecast_df["timestamp"])
    forecast_df["time"] = forecast_df["timestamp"].dt.strftime("%I:%M %p")

    # Display table
    st.dataframe(
        forecast_df[["time", "predicted_demand", "confidence_interval_lower", "confidence_interval_upper"]].rename(columns={
            "time": "Time",
            "predicted_demand": "Predicted Bikes",
            "confidence_interval_lower": "CI Lower",
            "confidence_interval_upper": "CI Upper"
        }),
        use_container_width=True,
        hide_index=True
    )

    # Download forecast
    csv = forecast_df.to_csv(index=False)
    st.download_button(
        label="ğŸ“¥ Download Forecast (CSV)",
        data=csv,
        file_name=f"forecast_{selected_station_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
        mime="text/csv"
    )
else:
    st.error("âŒ Failed to load forecast data")

st.markdown("---")

# Weekly pattern heatmap
st.subheader("ğŸ“… Historical Demand Pattern")
fig_heatmap = create_demand_heatmap(selected_station_id)
st.plotly_chart(fig_heatmap, use_container_width=True)

# Insights
st.markdown("---")
st.subheader("ğŸ’¡ Insights & Recommendations")

# Simple logic-based insights (can be enhanced with ML)
insights = []

if utilization > 80:
    insights.append("ğŸ”´ **High Demand**: Station is nearly full. Consider rebalancing bikes.")
elif utilization < 20:
    insights.append("ğŸŸ¡ **Low Demand**: Few bikes available. May need bike delivery.")
else:
    insights.append("ğŸŸ¢ **Normal Demand**: Station operating within normal range.")

# Check forecast for capacity issues
if forecast_data:
    max_predicted = max([f["predicted_demand"] for f in forecast_data["forecasts"]])
    if max_predicted > current_status["capacity"] * 0.9:
        insights.append("âš ï¸ **Capacity Warning**: Forecast shows potential capacity issues in next 6 hours.")

    min_predicted = min([f["predicted_demand"] for f in forecast_data["forecasts"]])
    if min_predicted < 2:
        insights.append("âš ï¸ **Low Stock Warning**: Station may run out of bikes soon.")

for insight in insights:
    st.info(insight)
```

**Key Features**:
- **Caching**: `@st.cache_data` reduces API calls (5-min cache for stations, 1-min for forecasts)
- **Auto-refresh**: Optional 30-second auto-reload for monitoring
- **Interactive charts**: Plotly for zoomable, hoverable visualizations
- **Confidence intervals**: Shaded area shows prediction uncertainty
- **Demand heatmap**: Weekly pattern visualization
- **Insights**: Logic-based recommendations for operations

---

## Step 3: Model Performance Page

### `dashboard/pages/2_ğŸ¯_model_performance.py`

```python
import streamlit as st
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime, timedelta
import psycopg2
from psycopg2.extras import RealDictCursor
import os

st.set_page_config(page_title="Model Performance", page_icon="ğŸ¯", layout="wide")

st.title("ğŸ¯ Model Performance Monitoring")
st.markdown("Track model accuracy, performance trends, and model comparison")

# --- Database Connection ---

@st.cache_resource
def get_db_connection():
    """Create PostgreSQL connection"""
    return psycopg2.connect(
        host=os.getenv("DB_HOST", "localhost"),
        port=os.getenv("DB_PORT", 5432),
        database=os.getenv("DB_NAME", "bike_demand_db"),
        user=os.getenv("DB_USER", "postgres"),
        password=os.getenv("DB_PASSWORD", "postgres")
    )

@st.cache_data(ttl=300)
def fetch_latest_metrics() -> Dict:
    """Fetch latest model performance metrics"""
    conn = get_db_connection()
    cursor = conn.cursor(cursor_factory=RealDictCursor)

    query = """
    SELECT
        model_name,
        model_version,
        rmse,
        mae,
        mape,
        r2_score,
        evaluation_date
    FROM model_performance
    WHERE model_name = 'bike-demand-forecaster'
    ORDER BY evaluation_date DESC
    LIMIT 1
    """

    cursor.execute(query)
    result = cursor.fetchone()
    cursor.close()

    return dict(result) if result else None

@st.cache_data(ttl=300)
def fetch_performance_history(days: int = 30) -> pd.DataFrame:
    """Fetch model performance over time"""
    conn = get_db_connection()

    query = """
    SELECT
        evaluation_date,
        model_name,
        rmse,
        mae,
        mape,
        r2_score
    FROM model_performance
    WHERE evaluation_date >= NOW() - INTERVAL '%s days'
    ORDER BY evaluation_date ASC
    """

    df = pd.read_sql_query(query, conn, params=(days,))
    return df

@st.cache_data(ttl=300)
def fetch_model_comparison() -> pd.DataFrame:
    """Compare different model versions"""
    conn = get_db_connection()

    query = """
    SELECT
        model_name,
        model_version,
        AVG(rmse) as avg_rmse,
        AVG(mae) as avg_mae,
        AVG(mape) as avg_mape,
        AVG(r2_score) as avg_r2,
        COUNT(*) as evaluations
    FROM model_performance
    WHERE evaluation_date >= NOW() - INTERVAL '30 days'
    GROUP BY model_name, model_version
    ORDER BY avg_rmse ASC
    """

    df = pd.read_sql_query(query, conn)
    return df

# --- Main UI ---

# Latest Metrics
st.subheader("ğŸ“Š Current Model Performance")

latest_metrics = fetch_latest_metrics()

if latest_metrics:
    col1, col2, col3, col4, col5 = st.columns(5)

    with col1:
        st.metric(
            label="ğŸ¯ RMSE",
            value=f"{latest_metrics['rmse']:.3f}",
            delta=None,
            help="Root Mean Squared Error (lower is better)"
        )

    with col2:
        st.metric(
            label="ğŸ“‰ MAE",
            value=f"{latest_metrics['mae']:.3f}",
            delta=None,
            help="Mean Absolute Error (lower is better)"
        )

    with col3:
        st.metric(
            label="ğŸ“Š MAPE",
            value=f"{latest_metrics['mape']:.2f}%",
            delta=None,
            help="Mean Absolute Percentage Error (lower is better)"
        )

    with col4:
        st.metric(
            label="ğŸ“ˆ RÂ² Score",
            value=f"{latest_metrics['r2_score']:.3f}",
            delta=None,
            help="R-squared (higher is better, max 1.0)"
        )

    with col5:
        st.metric(
            label="ğŸ·ï¸ Model Version",
            value=latest_metrics['model_version'],
            delta=None
        )

    st.caption(f"Last evaluated: {latest_metrics['evaluation_date'].strftime('%Y-%m-%d %H:%M:%S')}")
else:
    st.warning("âš ï¸ No performance metrics available")

st.markdown("---")

# Performance Trends
st.subheader("ğŸ“ˆ Performance Trends (Last 30 Days)")

history_df = fetch_performance_history(days=30)

if not history_df.empty:
    # Create multi-metric chart
    fig = go.Figure()

    fig.add_trace(go.Scatter(
        x=history_df['evaluation_date'],
        y=history_df['rmse'],
        mode='lines+markers',
        name='RMSE',
        line=dict(color='#1f77b4', width=2)
    ))

    fig.add_trace(go.Scatter(
        x=history_df['evaluation_date'],
        y=history_df['mae'],
        mode='lines+markers',
        name='MAE',
        line=dict(color='#ff7f0e', width=2)
    ))

    fig.update_layout(
        title="Model Error Metrics Over Time",
        xaxis_title="Date",
        yaxis_title="Error Value",
        hovermode='x unified',
        template='plotly_white',
        height=400,
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
    )

    st.plotly_chart(fig, use_container_width=True)

    # RÂ² Score trend
    fig_r2 = go.Figure()

    fig_r2.add_trace(go.Scatter(
        x=history_df['evaluation_date'],
        y=history_df['r2_score'],
        mode='lines+markers',
        name='RÂ² Score',
        line=dict(color='#2ca02c', width=2),
        fill='tonexty'
    ))

    fig_r2.update_layout(
        title="RÂ² Score Over Time (Model Fit Quality)",
        xaxis_title="Date",
        yaxis_title="RÂ² Score",
        hovermode='x unified',
        template='plotly_white',
        height=300,
        yaxis=dict(range=[0, 1])
    )

    st.plotly_chart(fig_r2, use_container_width=True)
else:
    st.info("No performance history available yet.")

st.markdown("---")

# Model Comparison
st.subheader("âš–ï¸ Model Version Comparison")

comparison_df = fetch_model_comparison()

if not comparison_df.empty:
    # Bar chart for RMSE comparison
    fig_comparison = px.bar(
        comparison_df,
        x='model_version',
        y='avg_rmse',
        color='model_name',
        title="Average RMSE by Model Version (Last 30 Days)",
        labels={'avg_rmse': 'Average RMSE', 'model_version': 'Model Version'},
        text='avg_rmse'
    )

    fig_comparison.update_traces(texttemplate='%{text:.3f}', textposition='outside')
    fig_comparison.update_layout(height=400)

    st.plotly_chart(fig_comparison, use_container_width=True)

    # Detailed comparison table
    st.dataframe(
        comparison_df.rename(columns={
            'model_name': 'Model',
            'model_version': 'Version',
            'avg_rmse': 'Avg RMSE',
            'avg_mae': 'Avg MAE',
            'avg_mape': 'Avg MAPE (%)',
            'avg_r2': 'Avg RÂ²',
            'evaluations': 'Evaluations'
        }),
        use_container_width=True,
        hide_index=True
    )
else:
    st.info("No model comparison data available.")

st.markdown("---")

# Performance Alerts
st.subheader("âš ï¸ Performance Alerts")

alerts = []

if latest_metrics:
    # Check for performance degradation
    if latest_metrics['rmse'] > 1.0:
        alerts.append("ğŸ”´ **Critical**: RMSE exceeds threshold (1.0). Model retraining recommended.")
    elif latest_metrics['rmse'] > 0.7:
        alerts.append("ğŸŸ¡ **Warning**: RMSE is elevated. Monitor closely.")

    if latest_metrics['r2_score'] < 0.7:
        alerts.append("ğŸŸ¡ **Warning**: RÂ² score below 0.7. Model fit may be degrading.")

    if latest_metrics['mape'] > 25:
        alerts.append("ğŸŸ¡ **Warning**: MAPE above 25%. Predictions may be inaccurate.")

if alerts:
    for alert in alerts:
        st.warning(alert)
else:
    st.success("âœ… All performance metrics within acceptable thresholds")

# Download performance report
if not history_df.empty:
    csv = history_df.to_csv(index=False)
    st.download_button(
        label="ğŸ“¥ Download Performance Report (CSV)",
        data=csv,
        file_name=f"model_performance_{datetime.now().strftime('%Y%m%d')}.csv",
        mime="text/csv"
    )
```

**Key Features**:
- **Real metrics from database**: Queries `model_performance` table
- **Time-series trends**: Track RMSE, MAE, MAPE, RÂ² over 30 days
- **Model comparison**: Compare different versions side-by-side
- **Automated alerts**: Threshold-based warnings for performance degradation
- **Downloadable reports**: Export metrics as CSV

---

## Step 4: Data Quality Page

### `dashboard/pages/3_ğŸ“Š_data_quality.py`

```python
import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import psycopg2
from psycopg2.extras import RealDictCursor
import os

st.set_page_config(page_title="Data Quality", page_icon="ğŸ“Š", layout="wide")

st.title("ğŸ“Š Data Quality Monitoring")
st.markdown("Monitor data pipeline health, freshness, and quality metrics")

# --- Database Connection ---

@st.cache_resource
def get_db_connection():
    return psycopg2.connect(
        host=os.getenv("DB_HOST", "localhost"),
        port=os.getenv("DB_PORT", 5432),
        database=os.getenv("DB_NAME", "bike_demand_db"),
        user=os.getenv("DB_USER", "postgres"),
        password=os.getenv("DB_PASSWORD", "postgres")
    )

@st.cache_data(ttl=60)
def check_data_freshness() -> Dict:
    """Check latest data timestamp for each table"""
    conn = get_db_connection()
    cursor = conn.cursor(cursor_factory=RealDictCursor)

    tables = {
        "bike_station_status": "timestamp",
        "weather_data": "timestamp",
        "features": "timestamp",
        "predictions": "prediction_timestamp"
    }

    freshness = {}

    for table, ts_col in tables.items():
        query = f"SELECT MAX({ts_col}) as latest_timestamp FROM {table}"
        cursor.execute(query)
        result = cursor.fetchone()

        if result and result['latest_timestamp']:
            minutes_ago = (datetime.now() - result['latest_timestamp']).total_seconds() / 60
            freshness[table] = {
                "latest": result['latest_timestamp'],
                "minutes_ago": minutes_ago,
                "status": "ğŸŸ¢" if minutes_ago < 30 else "ğŸŸ¡" if minutes_ago < 120 else "ğŸ”´"
            }
        else:
            freshness[table] = {
                "latest": None,
                "minutes_ago": None,
                "status": "ğŸ”´"
            }

    cursor.close()
    return freshness

@st.cache_data(ttl=300)
def get_missing_data_stats() -> pd.DataFrame:
    """Calculate missing data percentages"""
    conn = get_db_connection()

    query = """
    SELECT
        DATE(timestamp) as date,
        COUNT(*) as total_records,
        SUM(CASE WHEN bikes_available IS NULL THEN 1 ELSE 0 END) as missing_bikes,
        SUM(CASE WHEN docks_available IS NULL THEN 1 ELSE 0 END) as missing_docks
    FROM bike_station_status
    WHERE timestamp >= NOW() - INTERVAL '7 days'
    GROUP BY DATE(timestamp)
    ORDER BY date DESC
    """

    df = pd.read_sql_query(query, conn)
    df['missing_bikes_pct'] = (df['missing_bikes'] / df['total_records']) * 100
    df['missing_docks_pct'] = (df['missing_docks'] / df['total_records']) * 100

    return df

@st.cache_data(ttl=300)
def get_data_volume_stats() -> pd.DataFrame:
    """Get data ingestion volume over time"""
    conn = get_db_connection()

    query = """
    SELECT
        DATE(timestamp) as date,
        COUNT(*) as records
    FROM bike_station_status
    WHERE timestamp >= NOW() - INTERVAL '30 days'
    GROUP BY DATE(timestamp)
    ORDER BY date ASC
    """

    df = pd.read_sql_query(query, conn)
    return df

# --- Main UI ---

# Data Freshness
st.subheader("ğŸ•’ Data Freshness")

freshness_data = check_data_freshness()

col1, col2, col3, col4 = st.columns(4)

columns = [col1, col2, col3, col4]
for i, (table, data) in enumerate(freshness_data.items()):
    with columns[i]:
        if data['latest']:
            minutes = int(data['minutes_ago'])
            st.metric(
                label=f"{data['status']} {table.replace('_', ' ').title()}",
                value=f"{minutes} min ago",
                delta=None
            )
        else:
            st.metric(
                label=f"{data['status']} {table.replace('_', ' ').title()}",
                value="No data",
                delta=None
            )

st.markdown("---")

# Missing Data
st.subheader("ğŸ” Missing Data Analysis (Last 7 Days)")

missing_df = get_missing_data_stats()

if not missing_df.empty:
    fig = go.Figure()

    fig.add_trace(go.Bar(
        x=missing_df['date'],
        y=missing_df['missing_bikes_pct'],
        name='Missing Bikes (%)',
        marker_color='#ff7f0e'
    ))

    fig.add_trace(go.Bar(
        x=missing_df['date'],
        y=missing_df['missing_docks_pct'],
        name='Missing Docks (%)',
        marker_color='#1f77b4'
    ))

    fig.update_layout(
        title="Missing Data Percentage by Day",
        xaxis_title="Date",
        yaxis_title="Missing Data (%)",
        barmode='group',
        height=400,
        template='plotly_white'
    )

    st.plotly_chart(fig, use_container_width=True)

    # Summary table
    st.dataframe(missing_df, use_container_width=True, hide_index=True)
else:
    st.info("No missing data statistics available.")

st.markdown("---")

# Data Volume
st.subheader("ğŸ“ˆ Data Ingestion Volume (Last 30 Days)")

volume_df = get_data_volume_stats()

if not volume_df.empty:
    fig_volume = px.area(
        volume_df,
        x='date',
        y='records',
        title="Daily Record Count",
        labels={'records': 'Number of Records', 'date': 'Date'}
    )

    fig_volume.update_layout(height=400, template='plotly_white')
    st.plotly_chart(fig_volume, use_container_width=True)

    # Statistics
    col1, col2, col3 = st.columns(3)

    with col1:
        st.metric("Avg Daily Records", f"{volume_df['records'].mean():.0f}")

    with col2:
        st.metric("Max Daily Records", f"{volume_df['records'].max():.0f}")

    with col3:
        st.metric("Min Daily Records", f"{volume_df['records'].min():.0f}")
else:
    st.info("No volume statistics available.")

st.markdown("---")

# Data Quality Alerts
st.subheader("âš ï¸ Data Quality Alerts")

alerts = []

# Check freshness
for table, data in freshness_data.items():
    if data['status'] == "ğŸ”´":
        alerts.append(f"ğŸ”´ **Critical**: {table} data is stale (>{int(data['minutes_ago'])} min old)")
    elif data['status'] == "ğŸŸ¡":
        alerts.append(f"ğŸŸ¡ **Warning**: {table} data delayed ({int(data['minutes_ago'])} min old)")

# Check missing data
if not missing_df.empty:
    latest_missing = missing_df.iloc[0]
    if latest_missing['missing_bikes_pct'] > 5:
        alerts.append(f"ğŸŸ¡ **Warning**: {latest_missing['missing_bikes_pct']:.1f}% missing bikes data today")
    if latest_missing['missing_docks_pct'] > 5:
        alerts.append(f"ğŸŸ¡ **Warning**: {latest_missing['missing_docks_pct']:.1f}% missing docks data today")

# Check volume
if not volume_df.empty:
    recent_avg = volume_df.tail(3)['records'].mean()
    overall_avg = volume_df['records'].mean()

    if recent_avg < overall_avg * 0.7:
        alerts.append(f"ğŸ”´ **Critical**: Data ingestion volume dropped by {((1 - recent_avg/overall_avg) * 100):.0f}%")

if alerts:
    for alert in alerts:
        st.warning(alert)
else:
    st.success("âœ… All data quality checks passed")
```

**Key Features**:
- **Data freshness tracking**: Checks latest timestamps for all tables
- **Missing data analysis**: Tracks NULL values over time
- **Volume monitoring**: Detects ingestion pipeline issues
- **Automated alerts**: Flags stale data, missing values, volume drops

---

## Step 5: System Health Page

### `dashboard/pages/4_ğŸ’š_system_health.py`

```python
import streamlit as st
import requests
from datetime import datetime
import psycopg2
import os

st.set_page_config(page_title="System Health", page_icon="ğŸ’š", layout="wide")

st.title("ğŸ’š System Health Dashboard")
st.markdown("Monitor API, database, and model serving status")

# Configuration
API_BASE_URL = os.getenv("API_BASE_URL", "http://localhost:8000")
DB_HOST = os.getenv("DB_HOST", "localhost")
DB_PORT = os.getenv("DB_PORT", 5432)

# --- Health Checks ---

def check_api_health() -> Dict:
    """Check FastAPI health"""
    try:
        response = requests.get(f"{API_BASE_URL}/health", timeout=5)
        if response.status_code == 200:
            data = response.json()
            return {"status": "âœ… Healthy", "latency": response.elapsed.total_seconds() * 1000, "details": data}
        else:
            return {"status": "ğŸ”´ Unhealthy", "latency": None, "details": None}
    except Exception as e:
        return {"status": f"ğŸ”´ Error: {str(e)}", "latency": None, "details": None}

def check_database_health() -> Dict:
    """Check PostgreSQL connection"""
    try:
        conn = psycopg2.connect(
            host=DB_HOST,
            port=DB_PORT,
            database="bike_demand_db",
            user="postgres",
            password="postgres",
            connect_timeout=5
        )
        cursor = conn.cursor()
        cursor.execute("SELECT 1")
        cursor.close()
        conn.close()
        return {"status": "âœ… Connected", "details": f"{DB_HOST}:{DB_PORT}"}
    except Exception as e:
        return {"status": f"ğŸ”´ Error: {str(e)}", "details": None}

def check_model_status() -> Dict:
    """Check model serving status"""
    try:
        response = requests.get(f"{API_BASE_URL}/models/current", timeout=5)
        if response.status_code == 200:
            model_info = response.json()
            return {"status": "âœ… Loaded", "details": model_info}
        else:
            return {"status": "ğŸ”´ Not loaded", "details": None}
    except Exception as e:
        return {"status": f"ğŸ”´ Error: {str(e)}", "details": None}

# --- Main UI ---

st.subheader("ğŸ” Component Status")

# API Health
api_health = check_api_health()
col1, col2 = st.columns([1, 3])

with col1:
    st.metric("ğŸŒ API Status", api_health["status"])

with col2:
    if api_health["latency"]:
        st.metric("âš¡ Response Time", f"{api_health['latency']:.0f} ms")
    if api_health["details"]:
        with st.expander("View API Details"):
            st.json(api_health["details"])

# Database Health
db_health = check_database_health()
col1, col2 = st.columns([1, 3])

with col1:
    st.metric("ğŸ—„ï¸ Database Status", db_health["status"])

with col2:
    if db_health["details"]:
        st.info(f"Connected to: {db_health['details']}")

# Model Health
model_health = check_model_status()
col1, col2 = st.columns([1, 3])

with col1:
    st.metric("ğŸ¤– Model Status", model_health["status"])

with col2:
    if model_health["details"]:
        with st.expander("View Model Details"):
            st.json(model_health["details"])

st.markdown("---")

# Overall Status
st.subheader("ğŸ¯ Overall System Status")

all_healthy = all([
    "âœ…" in api_health["status"],
    "âœ…" in db_health["status"],
    "âœ…" in model_health["status"]
])

if all_healthy:
    st.success("ğŸŸ¢ **All systems operational**")
else:
    st.error("ğŸ”´ **System degradation detected**")

# Refresh button
if st.button("ğŸ”„ Refresh Health Checks"):
    st.rerun()
```

**Key Features**:
- **API health check**: Tests `/health` endpoint
- **Database connectivity**: Verifies PostgreSQL connection
- **Model status**: Confirms model is loaded and ready
- **Overall status**: Single-glance system health view

---

## Step 6: Shared Components

### `dashboard/components/api_client.py`

```python
"""Reusable API client for dashboard pages"""

import requests
from typing import Dict, List, Optional
import streamlit as st

class BikeAPIClient:
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.session = requests.Session()

    def get_stations(self) -> List[Dict]:
        """Fetch all bike stations"""
        response = self.session.get(f"{self.base_url}/stations", timeout=10)
        response.raise_for_status()
        return response.json()

    def get_forecast(self, station_id: str, hours: int = 6) -> Dict:
        """Fetch demand forecast"""
        response = self.session.get(
            f"{self.base_url}/predict/station/{station_id}/forecast",
            params={"hours": hours},
            timeout=10
        )
        response.raise_for_status()
        return response.json()

    def get_prediction(self, station_id: str) -> Dict:
        """Get single prediction"""
        response = self.session.post(
            f"{self.base_url}/predict",
            json={"station_id": station_id},
            timeout=10
        )
        response.raise_for_status()
        return response.json()
```

---

## Step 7: Docker Deployment

### `docker/dashboard/Dockerfile`

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install dependencies
COPY pyproject.toml .
RUN pip install --no-cache-dir streamlit plotly pandas psycopg2-binary requests python-dotenv

# Copy dashboard code
COPY dashboard/ ./dashboard/

# Expose Streamlit port
EXPOSE 8501

# Health check
HEALTHCHECK CMD curl --fail http://localhost:8501/_stcore/health || exit 1

# Run Streamlit
CMD ["streamlit", "run", "dashboard/app.py", "--server.port=8501", "--server.address=0.0.0.0"]
```

### Update `infrastructure/docker-compose.yml`

```yaml
services:
  # ... existing services ...

  dashboard:
    build:
      context: ..
      dockerfile: docker/dashboard/Dockerfile
    ports:
      - "8501:8501"
    environment:
      - API_BASE_URL=http://api:8000
      - DB_HOST=postgres
      - DB_PORT=5432
      - DB_NAME=bike_demand_db
      - DB_USER=postgres
      - DB_PASSWORD=postgres
    depends_on:
      - api
      - postgres
    networks:
      - bike-demand-network
```

---

## Testing the Dashboard

### 1. Start All Services

```bash
cd infrastructure
docker-compose up -d
```

### 2. Access Dashboard

Open browser: `http://localhost:8501`

### 3. Test Navigation

- Click through all 4 pages in sidebar
- Verify data loads without errors
- Test forecast generation
- Check charts render correctly

### 4. Test Auto-Refresh

- Enable auto-refresh on Demand Forecast page
- Verify page updates every 30 seconds

---

## Performance Optimization

### 1. Caching Strategy

```python
# Cache station list for 5 minutes
@st.cache_data(ttl=300)
def fetch_stations():
    # Expensive API call
    pass

# Cache forecast for 1 minute
@st.cache_data(ttl=60)
def fetch_forecast(station_id, hours):
    # Time-sensitive data
    pass
```

### 2. Database Connection Pooling

```python
# Reuse connection across pages
@st.cache_resource
def get_db_connection():
    return psycopg2.connect(...)
```

### 3. Lazy Loading

```python
# Only load charts when page is selected
if st.session_state.get('current_page') == 'forecast':
    load_forecast_charts()
```

---

## Common Issues & Fixes

### Issue: "Connection refused" to API

**Cause**: API container not running or wrong URL

**Fix**:
```bash
# Check API is running
docker ps | grep api

# Check API health
curl http://localhost:8000/health

# Update API_BASE_URL in .env
API_BASE_URL=http://api:8000  # Inside Docker
API_BASE_URL=http://localhost:8000  # Local development
```

### Issue: Charts not rendering

**Cause**: Plotly not installed or data format issue

**Fix**:
```bash
pip install plotly

# Ensure data is DataFrame or dict, not None
if data is not None:
    st.plotly_chart(create_chart(data))
```

### Issue: Database connection errors

**Cause**: PostgreSQL credentials or host incorrect

**Fix**:
```python
# Check connection string
conn = psycopg2.connect(
    host="localhost",  # or "postgres" in Docker
    port=5432,
    database="bike_demand_db",
    user="postgres",
    password="postgres"
)
```

---

## Interview Talking Points

1. **"I built a multi-page Streamlit dashboard with real-time forecasting, model performance monitoring, and data quality tracking"**

2. **"I implemented caching strategies to optimize API calls - 5-minute cache for static data, 1-minute for forecasts"**

3. **"The dashboard integrates with FastAPI backend and PostgreSQL directly, providing both real-time predictions and historical analysis"**

4. **"I used Plotly for interactive visualizations with zoom, hover, and export capabilities"**

5. **"The system includes automated alerts for model degradation, data staleness, and system health issues"**

---

## Summary

You've built a production-ready dashboard with:

âœ… **Multi-page navigation** (4 pages: Forecast, Performance, Quality, Health)
âœ… **Real-time forecasts** with confidence intervals
âœ… **Model performance tracking** (RMSE, MAE, MAPE, RÂ²)
âœ… **Data quality monitoring** (freshness, missing data, volume)
âœ… **System health checks** (API, database, model status)
âœ… **Interactive Plotly charts** (zoomable, exportable)
âœ… **Automated alerts** (threshold-based warnings)
âœ… **Docker deployment** (containerized with docker-compose)
âœ… **Performance optimization** (caching, connection pooling)

**Next Chapter**: [Chapter 8: Orchestration with Airflow](08-airflow.md) - Automate data ingestion, feature engineering, and model training with Apache Airflow DAGs.

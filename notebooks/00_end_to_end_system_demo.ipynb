{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üö≤ Bike Demand Prediction System - Complete Demo\n",
    "\n",
    "**Level 2 MLOps Portfolio Project**\n",
    "\n",
    "This notebook demonstrates the complete bike demand forecasting system including:\n",
    "- üìä **Data Pipeline**: Real-time data collection from NYC Citi Bike & Weather APIs\n",
    "- üîß **Feature Engineering**: 100+ automated features (temporal, lag, rolling, weather)\n",
    "- ü§ñ **ML Training**: XGBoost, LightGBM, CatBoost with MLflow tracking\n",
    "- üìà **Predictions**: Single, batch, and multi-hour forecasts\n",
    "- üîç **Monitoring**: Data drift detection and model performance tracking\n",
    "\n",
    "---\n",
    "\n",
    "## System Architecture\n",
    "\n",
    "```\n",
    "APIs (Citi Bike + Weather)\n",
    "    ‚Üì\n",
    "Data Collection ‚Üí PostgreSQL ‚Üí Feature Engineering ‚Üí Feature Store\n",
    "                                                           ‚Üì\n",
    "                                      Model Training ‚Üí MLflow Registry\n",
    "                                                           ‚Üì\n",
    "                                      FastAPI + Streamlit Dashboard\n",
    "                                                           ‚Üì\n",
    "                                      Prometheus + Grafana Monitoring\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "# Data science\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"bike-demand-notebook-demo\")\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ Imports successful\")\n",
    "print(f\"üìÅ Project root: {project_root}\")\n",
    "print(f\"üìä MLflow tracking: http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Collection Pipeline\n",
    "\n",
    "### 2.1 Collect Bike Station Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.collectors.citi_bike_collector import CitiBikeCollector\n",
    "from src.data.collectors.weather_collector import WeatherCollector\n",
    "\n",
    "# Initialize collectors\n",
    "bike_collector = CitiBikeCollector()\n",
    "weather_collector = WeatherCollector()\n",
    "\n",
    "print(\"üö≤ Collecting bike station data from NYC Citi Bike API...\")\n",
    "\n",
    "with bike_collector:\n",
    "    # Collect station information\n",
    "    stations = bike_collector.collect_station_information()\n",
    "    print(f\"‚úÖ Collected {len(stations)} bike stations\")\n",
    "    \n",
    "    # Collect current status\n",
    "    statuses = bike_collector.collect_station_status()\n",
    "    print(f\"‚úÖ Collected {len(statuses)} station statuses\")\n",
    "\n",
    "# Convert to DataFrames\n",
    "df_stations = pd.DataFrame(stations)\n",
    "df_statuses = pd.DataFrame(statuses)\n",
    "\n",
    "print(f\"\\nüìä Station data shape: {df_stations.shape}\")\n",
    "print(f\"üìä Status data shape: {df_statuses.shape}\")\n",
    "\n",
    "# Preview\n",
    "display(df_stations.head())\n",
    "display(df_statuses.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Collect Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üå§Ô∏è Collecting weather data from OpenWeatherMap API...\")\n",
    "\n",
    "with weather_collector:\n",
    "    weather = weather_collector.collect_current_weather()\n",
    "    print(f\"‚úÖ Collected weather data\")\n",
    "\n",
    "# Display weather\n",
    "print(f\"\\nüìä Current Weather in NYC:\")\n",
    "print(f\"  üå°Ô∏è Temperature: {weather['temperature']}¬∞C\")\n",
    "print(f\"  üíß Humidity: {weather['humidity']}%\")\n",
    "print(f\"  üí® Wind Speed: {weather['wind_speed']} m/s\")\n",
    "print(f\"  ‚òÅÔ∏è Condition: {weather['weather_condition']}\")\n",
    "print(f\"  üìç Location: ({weather['latitude']}, {weather['longitude']})\")\n",
    "\n",
    "df_weather = pd.DataFrame([weather])\n",
    "display(df_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Save Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directories\n",
    "data_dir = project_root / \"data\"\n",
    "raw_dir = data_dir / \"raw\"\n",
    "processed_dir = data_dir / \"processed\"\n",
    "\n",
    "raw_dir.mkdir(parents=True, exist_ok=True)\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save raw data with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "stations_file = raw_dir / f\"stations_{timestamp}.csv\"\n",
    "statuses_file = raw_dir / f\"statuses_{timestamp}.csv\"\n",
    "weather_file = raw_dir / f\"weather_{timestamp}.csv\"\n",
    "\n",
    "df_stations.to_csv(stations_file, index=False)\n",
    "df_statuses.to_csv(statuses_file, index=False)\n",
    "df_weather.to_csv(weather_file, index=False)\n",
    "\n",
    "print(f\"üíæ Saved raw data to:\")\n",
    "print(f\"  üìÑ {stations_file}\")\n",
    "print(f\"  üìÑ {statuses_file}\")\n",
    "print(f\"  üìÑ {weather_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge station info with status\n",
    "df_merged = df_statuses.merge(df_stations, on='station_id', how='left')\n",
    "\n",
    "# Calculate demand\n",
    "df_merged['total_capacity'] = df_merged['bikes_available'] + df_merged['docks_available']\n",
    "df_merged['utilization'] = df_merged['bikes_available'] / df_merged['total_capacity']\n",
    "\n",
    "# Statistics\n",
    "print(\"üìä Station Statistics:\")\n",
    "print(f\"  Total Stations: {len(df_merged)}\")\n",
    "print(f\"  Active Stations: {df_merged['is_installed'].sum()}\")\n",
    "print(f\"  Total Bikes Available: {df_merged['bikes_available'].sum()}\")\n",
    "print(f\"  Total Docks Available: {df_merged['docks_available'].sum()}\")\n",
    "print(f\"  Average Utilization: {df_merged['utilization'].mean():.2%}\")\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Bikes available distribution\n",
    "axes[0, 0].hist(df_merged['bikes_available'], bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_title('Distribution of Bikes Available', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Bikes Available')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# 2. Utilization distribution\n",
    "axes[0, 1].hist(df_merged['utilization'].dropna(), bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[0, 1].set_title('Station Utilization Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Utilization Rate')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# 3. Top 10 stations by bikes\n",
    "top_stations = df_merged.nlargest(10, 'bikes_available')[['name', 'bikes_available']]\n",
    "axes[1, 0].barh(range(len(top_stations)), top_stations['bikes_available'].values)\n",
    "axes[1, 0].set_yticks(range(len(top_stations)))\n",
    "axes[1, 0].set_yticklabels([name[:30] + '...' if len(name) > 30 else name for name in top_stations['name']], fontsize=8)\n",
    "axes[1, 0].set_title('Top 10 Stations by Bikes Available', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Bikes Available')\n",
    "axes[1, 0].invert_yaxis()\n",
    "\n",
    "# 4. Capacity vs bikes scatter\n",
    "axes[1, 1].scatter(df_merged['capacity'], df_merged['bikes_available'], alpha=0.5)\n",
    "axes[1, 1].set_title('Station Capacity vs Bikes Available', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Station Capacity')\n",
    "axes[1, 1].set_ylabel('Bikes Available')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(data_dir / 'exploratory_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüíæ Saved visualization to {data_dir / 'exploratory_analysis.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Generate 100+ features from raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.temporal_features import TemporalFeatureGenerator\n",
    "from src.features.lag_features import LagFeatureGenerator\n",
    "from src.features.rolling_features import RollingFeatureGenerator\n",
    "from src.features.weather_features import WeatherFeatureGenerator\n",
    "from src.features.holiday_features import HolidayFeatureGenerator\n",
    "\n",
    "# Initialize generators\n",
    "temporal_gen = TemporalFeatureGenerator()\n",
    "lag_gen = LagFeatureGenerator()\n",
    "rolling_gen = RollingFeatureGenerator()\n",
    "weather_gen = WeatherFeatureGenerator()\n",
    "holiday_gen = HolidayFeatureGenerator()\n",
    "\n",
    "print(\"üîß Generating features...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create Sample Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 30 days of hourly data for demonstration\n",
    "hours = 30 * 24\n",
    "dates = pd.date_range(start='2024-11-01', periods=hours, freq='h')\n",
    "\n",
    "# Simulate demand with daily/weekly patterns\n",
    "np.random.seed(42)\n",
    "base_demand = 15\n",
    "hour_effect = 5 * np.sin(2 * np.pi * np.arange(hours) / 24)  # Daily pattern\n",
    "day_effect = 3 * np.sin(2 * np.pi * np.arange(hours) / (24 * 7))  # Weekly pattern\n",
    "noise = np.random.normal(0, 2, hours)\n",
    "demand = base_demand + hour_effect + day_effect + noise\n",
    "demand = np.maximum(demand, 0)  # No negative demand\n",
    "\n",
    "# Create DataFrame\n",
    "df_timeseries = pd.DataFrame({\n",
    "    'station_id': 'demo_station_001',\n",
    "    'timestamp': dates,\n",
    "    'bikes_available': demand,\n",
    "    'docks_available': 30 - demand,\n",
    "    'temperature': 15 + 5 * np.sin(2 * np.pi * np.arange(hours) / 24) + np.random.normal(0, 2, hours),\n",
    "    'humidity': 60 + 10 * np.random.randn(hours),\n",
    "    'wind_speed': 5 + 2 * np.random.randn(hours),\n",
    "    'precipitation': np.random.choice([0, 0, 0, 0.5, 1.0], hours),\n",
    "    'weather_condition': np.random.choice(['Clear', 'Clouds', 'Rain'], hours, p=[0.6, 0.3, 0.1])\n",
    "})\n",
    "\n",
    "print(f\"‚úÖ Created time series data: {df_timeseries.shape}\")\n",
    "display(df_timeseries.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Generate Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚è∞ Generating temporal features...\")\n",
    "df_features = temporal_gen.generate(df_timeseries.copy())\n",
    "\n",
    "temporal_cols = [col for col in df_features.columns if col not in df_timeseries.columns]\n",
    "print(f\"‚úÖ Generated {len(temporal_cols)} temporal features\")\n",
    "print(f\"   Features: {', '.join(temporal_cols[:10])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Generate Lag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚èÆÔ∏è Generating lag features...\")\n",
    "df_features = lag_gen.generate(df_features)\n",
    "\n",
    "lag_cols = [col for col in df_features.columns if 'lag_' in col or 'change_' in col]\n",
    "print(f\"‚úÖ Generated {len(lag_cols)} lag features\")\n",
    "print(f\"   Features: {', '.join(lag_cols[:10])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Generate Rolling Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Generating rolling window features...\")\n",
    "df_features = rolling_gen.generate(df_features)\n",
    "\n",
    "rolling_cols = [col for col in df_features.columns if 'rolling_' in col]\n",
    "print(f\"‚úÖ Generated {len(rolling_cols)} rolling features\")\n",
    "print(f\"   Features: {', '.join(rolling_cols[:10])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Generate Weather Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üå§Ô∏è Generating weather features...\")\n",
    "df_features = weather_gen.generate(df_features)\n",
    "\n",
    "weather_cols = [col for col in df_features.columns if any(x in col for x in ['temp_', 'humidity_', 'wind_', 'is_rainy', 'weather_severity'])]\n",
    "print(f\"‚úÖ Generated {len(weather_cols)} weather features\")\n",
    "print(f\"   Features: {', '.join(weather_cols[:10])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Generate Holiday Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéâ Generating holiday features...\")\n",
    "df_features = holiday_gen.generate(df_features)\n",
    "\n",
    "holiday_cols = [col for col in df_features.columns if 'holiday' in col]\n",
    "print(f\"‚úÖ Generated {len(holiday_cols)} holiday features\")\n",
    "print(f\"   Features: {holiday_cols}\")\n",
    "\n",
    "# Summary\n",
    "total_features = len(df_features.columns) - len(df_timeseries.columns)\n",
    "print(f\"\\nüéØ Total features generated: {total_features}\")\n",
    "print(f\"üìä Final dataset shape: {df_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Save Processed Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed features\n",
    "features_file = processed_dir / f\"features_{timestamp}.csv\"\n",
    "df_features.to_csv(features_file, index=False)\n",
    "print(f\"üíæ Saved features to: {features_file}\")\n",
    "\n",
    "# Display feature importance preview\n",
    "display(df_features.head())\n",
    "print(f\"\\nüìã All features ({len(df_features.columns)}):\")\n",
    "print(df_features.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "\n",
    "Train XGBoost, LightGBM, and CatBoost models with MLflow tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Drop NaN values (from lag/rolling features at the start)\n",
    "df_clean = df_features.dropna().reset_index(drop=True)\n",
    "\n",
    "# Define target and features\n",
    "target = 'bikes_available'\n",
    "exclude_cols = ['station_id', 'timestamp', target, 'docks_available', 'weather_condition']\n",
    "feature_cols = [col for col in df_clean.columns if col not in exclude_cols]\n",
    "\n",
    "X = df_clean[feature_cols]\n",
    "y = df_clean[target]\n",
    "\n",
    "print(f\"üìä Training data prepared:\")\n",
    "print(f\"   Samples: {len(X)}\")\n",
    "print(f\"   Features: {len(feature_cols)}\")\n",
    "print(f\"   Target: {target}\")\n",
    "\n",
    "# Time-based split (70% train, 15% val, 15% test)\n",
    "train_size = int(0.7 * len(X))\n",
    "val_size = int(0.15 * len(X))\n",
    "\n",
    "X_train = X[:train_size]\n",
    "y_train = y[:train_size]\n",
    "\n",
    "X_val = X[train_size:train_size + val_size]\n",
    "y_val = y[train_size:train_size + val_size]\n",
    "\n",
    "X_test = X[train_size + val_size:]\n",
    "y_test = y[train_size + val_size:]\n",
    "\n",
    "print(f\"\\nüìä Data splits:\")\n",
    "print(f\"   Train: {len(X_train)} samples ({len(X_train)/len(X):.1%})\")\n",
    "print(f\"   Val:   {len(X_val)} samples ({len(X_val)/len(X):.1%})\")\n",
    "print(f\"   Test:  {len(X_test)} samples ({len(X_test)/len(X):.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, set_name=\"Test\"):\n",
    "    \"\"\"Calculate and display metrics\"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\nüìä {set_name} Metrics:\")\n",
    "    print(f\"   RMSE: {rmse:.4f}\")\n",
    "    print(f\"   MAE:  {mae:.4f}\")\n",
    "    print(f\"   MAPE: {mape:.2f}%\")\n",
    "    print(f\"   R¬≤:   {r2:.4f}\")\n",
    "    \n",
    "    return {'rmse': rmse, 'mae': mae, 'mape': mape, 'r2': r2}\n",
    "\n",
    "def plot_predictions(y_true, y_pred, model_name, save_path=None):\n",
    "    \"\"\"Plot actual vs predicted\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Scatter plot\n",
    "    axes[0].scatter(y_true, y_pred, alpha=0.5)\n",
    "    axes[0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
    "    axes[0].set_xlabel('Actual')\n",
    "    axes[0].set_ylabel('Predicted')\n",
    "    axes[0].set_title(f'{model_name}: Actual vs Predicted')\n",
    "    \n",
    "    # Residuals\n",
    "    residuals = y_true - y_pred\n",
    "    axes[1].scatter(y_pred, residuals, alpha=0.5)\n",
    "    axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "    axes[1].set_xlabel('Predicted')\n",
    "    axes[1].set_ylabel('Residuals')\n",
    "    axes[1].set_title(f'{model_name}: Residual Plot')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Train XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ü§ñ Training XGBoost model...\\n\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"xgboost_demo\") as run:\n",
    "    # Model parameters\n",
    "    params_xgb = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.1,\n",
    "        'n_estimators': 100,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_params(params_xgb)\n",
    "    \n",
    "    # Train\n",
    "    model_xgb = xgb.XGBRegressor(**params_xgb)\n",
    "    model_xgb.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train = model_xgb.predict(X_train)\n",
    "    y_pred_val = model_xgb.predict(X_val)\n",
    "    y_pred_test = model_xgb.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    metrics_train = calculate_metrics(y_train, y_pred_train, \"Train\")\n",
    "    metrics_val = calculate_metrics(y_val, y_pred_val, \"Validation\")\n",
    "    metrics_test = calculate_metrics(y_test, y_pred_test, \"Test\")\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metrics({\n",
    "        'train_rmse': metrics_train['rmse'],\n",
    "        'val_rmse': metrics_val['rmse'],\n",
    "        'test_rmse': metrics_test['rmse'],\n",
    "        'test_mae': metrics_test['mae'],\n",
    "        'test_r2': metrics_test['r2']\n",
    "    })\n",
    "    \n",
    "    # Plot\n",
    "    plot_path = data_dir / 'xgboost_predictions.png'\n",
    "    plot_predictions(y_test, y_pred_test, \"XGBoost\", plot_path)\n",
    "    mlflow.log_artifact(str(plot_path))\n",
    "    \n",
    "    # Save model\n",
    "    models_dir = project_root / \"models\"\n",
    "    models_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    model_path = models_dir / f\"xgboost_{timestamp}.json\"\n",
    "    model_xgb.save_model(model_path)\n",
    "    print(f\"\\nüíæ Saved XGBoost model to: {model_path}\")\n",
    "    \n",
    "    # Log model to MLflow\n",
    "    mlflow.xgboost.log_model(model_xgb, \"model\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ XGBoost training complete!\")\n",
    "    print(f\"   MLflow Run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Train LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ü§ñ Training LightGBM model...\\n\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"lightgbm_demo\") as run:\n",
    "    # Model parameters\n",
    "    params_lgb = {\n",
    "        'objective': 'regression',\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.1,\n",
    "        'n_estimators': 100,\n",
    "        'random_state': 42,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_params(params_lgb)\n",
    "    \n",
    "    # Train\n",
    "    model_lgb = lgb.LGBMRegressor(**params_lgb)\n",
    "    model_lgb.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)]\n",
    "    )\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train = model_lgb.predict(X_train)\n",
    "    y_pred_val = model_lgb.predict(X_val)\n",
    "    y_pred_test = model_lgb.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    metrics_train = calculate_metrics(y_train, y_pred_train, \"Train\")\n",
    "    metrics_val = calculate_metrics(y_val, y_pred_val, \"Validation\")\n",
    "    metrics_test = calculate_metrics(y_test, y_pred_test, \"Test\")\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metrics({\n",
    "        'train_rmse': metrics_train['rmse'],\n",
    "        'val_rmse': metrics_val['rmse'],\n",
    "        'test_rmse': metrics_test['rmse'],\n",
    "        'test_mae': metrics_test['mae'],\n",
    "        'test_r2': metrics_test['r2']\n",
    "    })\n",
    "    \n",
    "    # Plot\n",
    "    plot_path = data_dir / 'lightgbm_predictions.png'\n",
    "    plot_predictions(y_test, y_pred_test, \"LightGBM\", plot_path)\n",
    "    mlflow.log_artifact(str(plot_path))\n",
    "    \n",
    "    # Save model\n",
    "    model_path = models_dir / f\"lightgbm_{timestamp}.txt\"\n",
    "    model_lgb.booster_.save_model(str(model_path))\n",
    "    print(f\"\\nüíæ Saved LightGBM model to: {model_path}\")\n",
    "    \n",
    "    # Log model to MLflow\n",
    "    mlflow.lightgbm.log_model(model_lgb, \"model\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ LightGBM training complete!\")\n",
    "    print(f\"   MLflow Run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Train CatBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ü§ñ Training CatBoost model...\\n\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"catboost_demo\") as run:\n",
    "    # Model parameters\n",
    "    params_cat = {\n",
    "        'iterations': 100,\n",
    "        'depth': 6,\n",
    "        'learning_rate': 0.1,\n",
    "        'random_state': 42,\n",
    "        'verbose': False\n",
    "    }\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_params(params_cat)\n",
    "    \n",
    "    # Train\n",
    "    model_cat = CatBoostRegressor(**params_cat)\n",
    "    model_cat.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=(X_val, y_val)\n",
    "    )\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train = model_cat.predict(X_train)\n",
    "    y_pred_val = model_cat.predict(X_val)\n",
    "    y_pred_test = model_cat.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    metrics_train = calculate_metrics(y_train, y_pred_train, \"Train\")\n",
    "    metrics_val = calculate_metrics(y_val, y_pred_val, \"Validation\")\n",
    "    metrics_test = calculate_metrics(y_test, y_pred_test, \"Test\")\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metrics({\n",
    "        'train_rmse': metrics_train['rmse'],\n",
    "        'val_rmse': metrics_val['rmse'],\n",
    "        'test_rmse': metrics_test['rmse'],\n",
    "        'test_mae': metrics_test['mae'],\n",
    "        'test_r2': metrics_test['r2']\n",
    "    })\n",
    "    \n",
    "    # Plot\n",
    "    plot_path = data_dir / 'catboost_predictions.png'\n",
    "    plot_predictions(y_test, y_pred_test, \"CatBoost\", plot_path)\n",
    "    mlflow.log_artifact(str(plot_path))\n",
    "    \n",
    "    # Save model\n",
    "    model_path = models_dir / f\"catboost_{timestamp}.cbm\"\n",
    "    model_cat.save_model(str(model_path))\n",
    "    print(f\"\\nüíæ Saved CatBoost model to: {model_path}\")\n",
    "    \n",
    "    # Log model to MLflow\n",
    "    mlflow.catboost.log_model(model_cat, \"model\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ CatBoost training complete!\")\n",
    "    print(f\"   MLflow Run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "print(\"üìä Model Comparison Summary\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"View detailed comparison in MLflow UI:\")\n",
    "print(\"üëâ http://localhost:5000\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüèÜ All models trained and logged to MLflow!\")\n",
    "print(f\"\\nüíæ Models saved in: {models_dir}\")\n",
    "print(f\"üìä Visualizations saved in: {data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Making Predictions\n",
    "\n",
    "Use the best model for forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use XGBoost for demo (typically you'd select based on metrics)\n",
    "best_model = model_xgb\n",
    "print(\"üéØ Using XGBoost model for predictions\\n\")\n",
    "\n",
    "# Single prediction\n",
    "sample_features = X_test.iloc[0:1]\n",
    "single_pred = best_model.predict(sample_features)[0]\n",
    "actual_value = y_test.iloc[0]\n",
    "\n",
    "print(f\"üìç Single Prediction Example:\")\n",
    "print(f\"   Predicted: {single_pred:.2f} bikes\")\n",
    "print(f\"   Actual:    {actual_value:.2f} bikes\")\n",
    "print(f\"   Error:     {abs(single_pred - actual_value):.2f} bikes\")\n",
    "\n",
    "# Batch predictions\n",
    "batch_size = 24  # 24 hours\n",
    "batch_features = X_test.iloc[:batch_size]\n",
    "batch_preds = best_model.predict(batch_features)\n",
    "batch_actuals = y_test.iloc[:batch_size].values\n",
    "\n",
    "print(f\"\\nüìä Batch Prediction (24 hours):\")\n",
    "print(f\"   Mean predicted: {batch_preds.mean():.2f} bikes\")\n",
    "print(f\"   Mean actual:    {batch_actuals.mean():.2f} bikes\")\n",
    "print(f\"   RMSE:          {np.sqrt(mean_squared_error(batch_actuals, batch_preds)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Forecast Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-day forecast visualization\n",
    "forecast_hours = 7 * 24  # 7 days\n",
    "forecast_features = X_test.iloc[:forecast_hours]\n",
    "forecast_preds = best_model.predict(forecast_features)\n",
    "forecast_actuals = y_test.iloc[:forecast_hours].values\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15, 6))\n",
    "hours_range = range(len(forecast_preds))\n",
    "\n",
    "plt.plot(hours_range, forecast_actuals, label='Actual', linewidth=2, alpha=0.7)\n",
    "plt.plot(hours_range, forecast_preds, label='Predicted', linewidth=2, alpha=0.7, linestyle='--')\n",
    "plt.fill_between(hours_range, forecast_actuals, forecast_preds, alpha=0.2)\n",
    "\n",
    "plt.xlabel('Hours Ahead', fontsize=12)\n",
    "plt.ylabel('Bikes Available', fontsize=12)\n",
    "plt.title('7-Day Demand Forecast: Actual vs Predicted', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "forecast_plot = data_dir / '7day_forecast.png'\n",
    "plt.savefig(forecast_plot, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"üíæ Saved forecast visualization to: {forecast_plot}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Monitoring & Analysis\n",
    "\n",
    "### 6.1 Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': best_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot top 20\n",
    "top_features = feature_importance.head(20)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(len(top_features)), top_features['importance'].values)\n",
    "plt.yticks(range(len(top_features)), top_features['feature'].values)\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.title('Top 20 Most Important Features', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "\n",
    "importance_plot = data_dir / 'feature_importance.png'\n",
    "plt.savefig(importance_plot, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"üíæ Saved feature importance plot to: {importance_plot}\")\n",
    "print(f\"\\nüîù Top 10 Features:\")\n",
    "for i, row in top_features.head(10).iterrows():\n",
    "    print(f\"   {row['feature']}: {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze errors by time of day\n",
    "test_df = df_clean.iloc[train_size + val_size:].copy()\n",
    "test_df['prediction'] = y_pred_test\n",
    "test_df['error'] = np.abs(test_df[target] - test_df['prediction'])\n",
    "\n",
    "# Error by hour\n",
    "if 'hour_of_day' in test_df.columns:\n",
    "    error_by_hour = test_df.groupby('hour_of_day')['error'].mean()\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.bar(error_by_hour.index, error_by_hour.values)\n",
    "    plt.xlabel('Hour of Day', fontsize=12)\n",
    "    plt.ylabel('Mean Absolute Error', fontsize=12)\n",
    "    plt.title('Prediction Error by Hour of Day', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(range(24))\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    error_plot = data_dir / 'error_by_hour.png'\n",
    "    plt.savefig(error_plot, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üíæ Saved error analysis to: {error_plot}\")\n",
    "    print(f\"\\nüìä Peak error hours:\")\n",
    "    print(error_by_hour.nlargest(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\" \"*20 + \"üéâ DEMO COMPLETE! üéâ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìä What We Accomplished:\")\n",
    "print(\"\\n1Ô∏è‚É£ Data Pipeline\")\n",
    "print(f\"   ‚úÖ Collected {len(df_stations)} bike stations from NYC Citi Bike API\")\n",
    "print(f\"   ‚úÖ Collected {len(df_statuses)} station statuses\")\n",
    "print(f\"   ‚úÖ Collected weather data from OpenWeatherMap\")\n",
    "print(f\"   ‚úÖ Saved raw data to: {raw_dir}\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ Feature Engineering\")\n",
    "print(f\"   ‚úÖ Generated {total_features} features\")\n",
    "print(f\"   ‚úÖ Created {hours} hours of time series data\")\n",
    "print(f\"   ‚úÖ Saved features to: {processed_dir}\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ Model Training\")\n",
    "print(f\"   ‚úÖ Trained 3 models: XGBoost, LightGBM, CatBoost\")\n",
    "print(f\"   ‚úÖ Logged all experiments to MLflow\")\n",
    "print(f\"   ‚úÖ Saved models to: {models_dir}\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ Predictions & Monitoring\")\n",
    "print(f\"   ‚úÖ Generated 7-day forecast\")\n",
    "print(f\"   ‚úÖ Analyzed feature importance\")\n",
    "print(f\"   ‚úÖ Performed error analysis\")\n",
    "print(f\"   ‚úÖ Saved visualizations to: {data_dir}\")\n",
    "\n",
    "print(\"\\nüîó Quick Links:\")\n",
    "print(f\"   üìä MLflow UI:       http://localhost:5000\")\n",
    "print(f\"   üöÄ FastAPI Docs:    http://localhost:8000/docs\")\n",
    "print(f\"   üìà Dashboard:       http://localhost:8501\")\n",
    "print(f\"   üìÅ Data Folder:     {data_dir}\")\n",
    "print(f\"   ü§ñ Models Folder:   {models_dir}\")\n",
    "\n",
    "print(\"\\nüéØ Next Steps:\")\n",
    "print(\"   1. Start Docker services: docker-compose up -d\")\n",
    "print(\"   2. View MLflow experiments: http://localhost:5000\")\n",
    "print(\"   3. Test FastAPI: http://localhost:8000/docs\")\n",
    "print(\"   4. Open Streamlit dashboard: http://localhost:8501\")\n",
    "print(\"   5. Set up Airflow DAGs for automation\")\n",
    "print(\"   6. Configure monitoring with Prometheus + Grafana\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üöÄ System is ready for deployment!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Bonus: Quick Model Deployment Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if API is running\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    response = requests.get('http://localhost:8000/health', timeout=2)\n",
    "    if response.status_code == 200:\n",
    "        print(\"‚úÖ FastAPI is running!\")\n",
    "        print(f\"   Response: {response.json()}\")\n",
    "        print(\"\\n   Try making a prediction at: http://localhost:8000/docs\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è FastAPI returned unexpected status\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"‚ÑπÔ∏è FastAPI not running. Start it with:\")\n",
    "    print(\"   python src/serving/api/main.py\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ÑπÔ∏è Could not connect to API: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- **Documentation**: See `docs/` folder\n",
    "- **API Guide**: `docs/API_QUICK_START.md`\n",
    "- **Deployment**: `docs/DEPLOYMENT.md`\n",
    "- **GitHub**: https://github.com/shima-maleki/Bike-Demand-Prediction-for-Smart-Cities\n",
    "\n",
    "---\n",
    "\n",
    "**Level 2 MLOps Portfolio Project**  \n",
    "*Automated Data Pipeline ‚Ä¢ Experiment Tracking ‚Ä¢ Model Registry ‚Ä¢ CI/CD ‚Ä¢ Monitoring*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bike-Demand-Prediction-for-Smart-Cities",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
